{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_hidden_layers\": 2,\n",
    "    \"num_hidden_units\": 400,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_samples\": 1,\n",
    "    \"pi\": 0.25,\n",
    "    \"sigma_p\": 1.0,\n",
    "    \"sigma_p1\": 0.75,\n",
    "    \"sigma_p2\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d95c0b020a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m126.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnum_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mx' is not defined"
     ]
    }
   ],
   "source": [
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/126.0, label.astype(np.float32)\n",
    "\n",
    "mnist = mx.test_utils.get_mnist()\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)\n",
    "\n",
    "num_train = sum([batch_size for i in train_data])\n",
    "num_batches = num_train / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ddfee8b5cda3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnum_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_train\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "batch_size = config['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_layers = config['num_hidden_layers']\n",
    "\n",
    "# define function for evaluating MLP\n",
    "def net(X, layer_params):\n",
    "    layer_input = X\n",
    "    for i in range(len(layer_params) // 2 - 2):\n",
    "        h_linear = nd.dot(layer_input, layer_params[2*i]) + layer_params[2*i + 1]\n",
    "        layer_input = relu(h_linear)\n",
    "    # last layer without ReLU\n",
    "    output = nd.dot(layer_input, layer_params[-2]) + layer_params[-1]\n",
    "    return output\n",
    "\n",
    "# define network weight shapes\n",
    "layer_param_shapes = []\n",
    "num_hidden = config['num_hidden_units']\n",
    "for i in range(num_layers + 1):\n",
    "    if i == 0: # input layer\n",
    "        W_shape = (num_inputs, num_hidden)\n",
    "        b_shape = (num_hidden,)\n",
    "    elif i == num_layers: # last layer\n",
    "        W_shape = (num_hidden, num_outputs)\n",
    "        b_shape = (num_outputs,)\n",
    "    else: # hidden layers\n",
    "        W_shape = (num_hidden, num_hidden)\n",
    "        b_shape = (num_hidden,)\n",
    "    layer_param_shapes.extend([W_shape, b_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_softmax_likelihood(yhat_linear, y):\n",
    "    return nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LOG2PI = np.log(2.0 * np.pi)\n",
    "\n",
    "def log_gaussian(x, mu, sigma):\n",
    "    return -0.5 * LOG2PI - nd.log(sigma) - (x - mu) ** 2 / (2 * sigma ** 2)\n",
    "\n",
    "def gaussian_prior(x):\n",
    "    sigma_p = nd.array([config['sigma_p']], ctx=ctx)\n",
    "\n",
    "    return nd.sum(log_gaussian(x, 0., sigma_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma):\n",
    "    scaling = 1.0 / nd.sqrt(2.0 * np.pi * (sigma ** 2))\n",
    "    bell = nd.exp(- (x - mu) ** 2 / (2.0 * sigma ** 2))\n",
    "\n",
    "    return scaling * bell\n",
    "\n",
    "def scale_mixture_prior(x):\n",
    "    sigma_p1 = nd.array([config['sigma_p1']], ctx=ctx)\n",
    "    sigma_p2 = nd.array([config['sigma_p2']], ctx=ctx)\n",
    "    pi = config['pi']\n",
    "\n",
    "    first_gaussian = pi * gaussian(x, 0., sigma_p1)\n",
    "    second_gaussian = (1 - pi) * gaussian(x, 0., sigma_p2)\n",
    "\n",
    "    return nd.log(first_gaussian + second_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_loss(output, label_one_hot, params, mus, sigmas, log_prior, log_likelihood):\n",
    "\n",
    "    # Calculate data likelihood\n",
    "    log_likelihood_sum = nd.sum(log_likelihood(output, label_one_hot))\n",
    "\n",
    "    # Calculate prior\n",
    "    log_prior_sum = sum([nd.sum(log_prior(param)) for param in params])\n",
    "\n",
    "    # Calculate variational posterior\n",
    "    log_var_posterior_sum = sum([nd.sum(log_gaussian(params[i], mus[i], sigmas[i])) for i in range(len(params))])\n",
    "\n",
    "    # Calculate total loss\n",
    "    return 1.0 / num_batches * (log_var_posterior_sum - log_prior_sum) - log_likelihood_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SAG(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net, layer_params):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data, layer_params)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-69288790c0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_param_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mrho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrho_offset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nd' is not defined"
     ]
    }
   ],
   "source": [
    "weight_scale = .1\n",
    "rho_offset = -3\n",
    "\n",
    "# initialize variational parameters; mean and variance for each weight\n",
    "mus = []\n",
    "rhos = []\n",
    "\n",
    "for shape in layer_param_shapes:\n",
    "    mu = nd.random_normal(shape=shape, ctx=ctx, scale=weight_scale)\n",
    "    rho = rho_offset + nd.zeros(shape=shape, ctx=ctx)\n",
    "    mus.append(mu)\n",
    "    rhos.append(rho)\n",
    "\n",
    "variational_params = mus + rhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in variational_params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_epsilons(param_shapes):\n",
    "    epsilons = [nd.random_normal(shape=shape, loc=0., scale=1.0, ctx=ctx) for shape in param_shapes]\n",
    "    return epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return nd.log(1. + nd.exp(x))\n",
    "\n",
    "def transform_rhos(rhos):\n",
    "    return [softplus(rho) for rho in rhos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_gaussian_samples(mus, sigmas, epsilons):\n",
    "    samples = []\n",
    "    for j in range(len(mus)):\n",
    "        samples.append(mus[j] + sigmas[j] * epsilons[j])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-55e61e0fd801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = config['epochs']\n",
    "learning_rate = config['learning_rate']\n",
    "smoothing_constant = .01\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "\n",
    "        with autograd.record():\n",
    "            # sample epsilons from standard normal\n",
    "            epsilons = sample_epsilons(layer_param_shapes)\n",
    "\n",
    "            # compute softplus for variance\n",
    "            sigmas = transform_rhos(rhos)\n",
    "\n",
    "            # obtain a sample from q(w|theta) by transforming the epsilons\n",
    "            layer_params = transform_gaussian_samples(mus, sigmas, epsilons)\n",
    "\n",
    "            # forward-propagate the batch\n",
    "            output = net(data, layer_params)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = combined_loss(output, label_one_hot, layer_params, mus, sigmas, gaussian_prior, log_softmax_likelihood)\n",
    "\n",
    "        # backpropagate for gradient calculation\n",
    "        loss.backward()\n",
    "\n",
    "        # apply stochastic gradient descent to variational parameters\n",
    "        SGD(variational_params, learning_rate)\n",
    "\n",
    "        # calculate moving loss for monitoring convergence\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net, mus)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net, mus)\n",
    "    train_acc.append(np.asscalar(train_accuracy))\n",
    "    test_acc.append(np.asscalar(test_accuracy))\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))\n",
    "\n",
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptech",
   "language": "python",
   "name": "deeptech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
